# Copyright (c) Microsoft. All rights reserved.

import subprocess
from logging import Logger
from typing import Any, Dict, List, Optional, Union

from semantic_kernel.connectors.ai.complete_request_settings import (
    CompleteRequestSettings, )
from semantic_kernel.connectors.ai.text_completion_client_base import (
    TextCompletionClientBase, )

from log import logger


class Llama70bGGUFCompletion(TextCompletionClientBase):

    def __init__(self) -> None:
        pass

    async def complete_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ) -> Union[str, List[str]]:
        logger.info(f": LLM Inference input - {repr(prompt)}")
        result = subprocess.run([
            '/home/lwk/llama.cpp/main', '-ngl', '81', '-m',
            '/home/lwk/.cache/huggingface/hub/models-TheBloke--llama-2-70b-chat-gguf/llama-2-70b-chat.Q4_K_M.gguf',
            '--color', '-c', '4096', '--temp', '0', '--repeat-penalty', '1.1',
            '-n', '-1', '-p', prompt
        ],
                                capture_output=True)
        logger.info(f": LLM Inference output - {repr(result.stdout.decode())}")
        if result.returncode != 0:
            logger.error(
                f": LLM Inference fail - {repr(result.stderr.decode())}")
        return result.stdout.decode()

    async def complete_stream_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ):
        raise Exception(
            "Llama70bGGUFCompletion does not support stream output.")
