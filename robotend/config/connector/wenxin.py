import json
import requests
from logging import Logger
from typing import List, Optional, Union

from semantic_kernel.connectors.ai.complete_request_settings import (
    CompleteRequestSettings,)
from semantic_kernel.connectors.ai.text_completion_client_base import (
    TextCompletionClientBase,)

from log import logger
from dotenv import dotenv_values

config = dotenv_values(".env")

API_KEY = config.get("WENXIN_API_KEY", None)
SECRET_KEY = config.get("WENIN_SECRET_KEY", None)
TOKEN_URL = config.get("WENXIN_TOKEN_URL", None)
QUERY_URL = config.get("WENXIN_4_0104_QUERY_URL", None)


class WenxinCompletion(TextCompletionClientBase):

    def __init__(self) -> None:
        pass

    async def complete_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ) -> Union[str, List[str]]:
        logger.info(f": LLM Inference input - {repr(prompt)}")
        # config.get()
        # result = subprocess.run([
        #     '/home/lwk/llama.cpp/main', '-ngl', '81', '-m',
        #     '/home/lwk/.cache/huggingface/hub/models-TheBloke--llama-2-70b-chat-gguf/llama-2-70b-chat.Q4_K_M.gguf',
        #     '--color', '-c', '4096', '--temp', '0', '--repeat-penalty', '1.1', '-n', '-1', '-p', prompt
        # ],
        #                         capture_output=True)
        params = {"grant_type": "client_credentials", "client_id": API_KEY, "client_secret": SECRET_KEY}
        access_token = str(requests.post(TOKEN_URL, params=params).json().get("access_token"))
        url = QUERY_URL + access_token
        payload = json.dumps({"messages": [{"role": "user", "content": prompt}], "temperature": 0.1})
        headers = {'Content-Type': 'application/json'}
        response = requests.request("POST", url, headers=headers, data=payload)
        result = json.loads(response.text)["result"]
        print("result:", result)
        logger.info(f": LLM Inference output - {repr(result)}")
        return result

    async def complete_stream_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ):
        raise Exception("WenXinCompletion does not support stream output.")
