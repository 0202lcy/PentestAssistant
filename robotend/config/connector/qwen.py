import json
import requests
from logging import Logger
from typing import List, Optional, Union

from semantic_kernel.connectors.ai.complete_request_settings import (
    CompleteRequestSettings,)
from semantic_kernel.connectors.ai.text_completion_client_base import (
    TextCompletionClientBase,)

from log import logger
from dotenv import dotenv_values

config = dotenv_values(".env")

API_KEY = config.get("QWEN_API_KEY", None)
api_key = "sk-a2e6af3778eb4d99a75c2a05be4fd8d3"


class QwenCompletion(TextCompletionClientBase):

    def __init__(self) -> None:
        pass

    async def complete_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ) -> Union[str, List[str]]:
        logger.info(f": LLM Inference input - {repr(prompt)}")
        # config.get()
        # result = subprocess.run([
        #     '/home/lwk/llama.cpp/main', '-ngl', '81', '-m',
        #     '/home/lwk/.cache/huggingface/hub/models-TheBloke--llama-2-70b-chat-gguf/llama-2-70b-chat.Q4_K_M.gguf',
        #     '--color', '-c', '4096', '--temp', '0', '--repeat-penalty', '1.1', '-n', '-1', '-p', prompt
        # ],
        #                         capture_output=True)
        url = 'https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation'
        headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}'}
        body = {
            'model': 'qwen-turbo',
            "input": {
                "messages": [{
                    "role": "system",
                    "content": "You are a helpful assistant."
                }, {
                    "role": "user",
                    "content": prompt
                }]
            },
            "parameters": {
                "result_format": "message"
            }
        }

        response = requests.post(url, headers=headers, json=body)
        result = response.json()['output']['choices'][0]['message']['content']
        print("result:", result)
        logger.info(f": LLM Inference output - {repr(result)}")
        return result

    async def complete_stream_async(
        self,
        prompt: str,
        request_settings: CompleteRequestSettings,
        logger: Optional[Logger] = logger,
    ):
        raise Exception("WenXinCompletion does not support stream output.")
